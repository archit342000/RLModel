{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment, Datastore,Environment\n",
        "from azureml.widgets import RunDetails\n",
        " \n",
        "from azureml.core import Dataset\n",
        " \n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        " \n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        " \n",
        "from azureml.core.runconfig import RunConfiguration, DockerConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        " \n",
        "from azureml.core.model import Model\n",
        "\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        " \n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.37.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1644316784492
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1644316785503
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def_blob_store = ws.get_default_datastore()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1644316785792
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aml_compute_target = \"RLModel\"\n",
        "try:\n",
        "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "    print(\"found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    print(\"creating new compute target\")\n",
        "    \n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_ND6S\",\n",
        "                                                                min_nodes = 0, \n",
        "                                                                max_nodes = 3)    \n",
        "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=60)\n",
        "    \n",
        "print(\"Azure Machine Learning Compute attached\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "found existing compute target.\nAzure Machine Learning Compute attached\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1644316786277
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aml_run_config = RunConfiguration()\n",
        "\n",
        "aml_run_config.docker = DockerConfiguration(use_docker=True)\n",
        "#DOCKER_ARGUMENTS = [\"--shm_size\",\"128g\"]  # increase shared memory\n",
        "#aml_run_config.environment.docker.arguments = DOCKER_ARGUMENTS\n",
        "\n",
        "aml_run_config.target = aml_compute\n",
        "#aml_run_config.environment.docker.enabled = True\n",
        "aml_run_config.environment.docker.base_image = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220113.v1\"\n",
        " \n",
        "aml_run_config.environment.python.user_managed_dependencies = False\n",
        " \n",
        "aml_run_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path='./environment.yml')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "<class 'azureml.core.runconfig.DockerConfiguration'> has no attribute base_image",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-07c88be0c210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#aml_run_config.environment.docker.enabled = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0maml_run_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDockerConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_docker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0maml_run_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220113.v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maml_run_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_managed_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/_base_sdk_common/abstract_run_config_element.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AbstractRunConfigElement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} has no attribute {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Just set it, as these are invoked from constructor before self._initialized=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: <class 'azureml.core.runconfig.DockerConfiguration'> has no attribute base_image"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1644315593559
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scriptNode(name,script,arguments = None, inputs=None,outputs=None,source=None):   \n",
        "    nodestep = PythonScriptStep(name=name, script_name=script, arguments = arguments,\n",
        "                         inputs = inputs, outputs = outputs, compute_target=aml_compute,\n",
        "                         runconfig=aml_run_config, source_directory=source,\n",
        "                         allow_reuse=True)\n",
        "    return nodestep"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644315593725
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#file_dataset = Dataset.File.upload_directory(src_dir=\"./pred_data\",target=def_blob_store)\n",
        "#file_dataset.register(workspace=ws,name='pred')\n",
        "\n",
        "#davis_data = Dataset.File.upload_directory(src_dir=\"./davis_data\",target=def_blob_store)\n",
        "#davis_data.register(workspace=ws,name='davis')\n",
        "\n",
        "#qsar_data = Dataset.File.upload_directory(src_dir=\"./qsar\",target=def_blob_store)\n",
        "#qsar_data.register(workspace=ws,name='qsar')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644315593914
        },
        "scrolled": true,
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moses = Dataset.get_by_name(ws, name='moses')\n",
        "pred_dataset = Dataset.get_by_name(ws, name='pred')\n",
        "davis_dataset = Dataset.get_by_name(ws, name='davis')\n",
        "qsar_dataset = Dataset.get_by_name(ws,\"qsar\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644315594289
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = moses.as_named_input('raw_data')\r\n",
        "in_data = PipelineData(\"Input_Data\", datastore=def_blob_store)\r\n",
        "\r\n",
        "train = PipelineData(\"training_processed\", datastore=def_blob_store)\r\n",
        "test = PipelineData(\"test_processed\", datastore=def_blob_store)\r\n",
        "valid = PipelineData(\"valid_processed\", datastore=def_blob_store)\r\n",
        "train_job_dir = PipelineData(\"train_job_dir\", datastore=def_blob_store)\r\n",
        "\r\n",
        "finetune_job_dir = PipelineData(\"finetune\", datastore=def_blob_store)\r\n",
        "\r\n",
        "qsar = qsar_dataset.as_named_input('qsar')\r\n",
        "\r\n",
        "generation = PipelineData(\"generation\", datastore=def_blob_store)\r\n",
        "\r\n",
        "modified_csv = PipelineData(\"csv\", datastore=def_blob_store)\r\n",
        "\r\n",
        "pred1 = pred_dataset.as_named_input('pred')\r\n",
        "\r\n",
        "davis1 = davis_dataset.as_named_input('davis')\r\n",
        "\r\n",
        "model_output = PipelineData(\"model\", datastore=def_blob_store)\r\n",
        "\r\n",
        "predict = PipelineData(\"predict\", datastore=def_blob_store)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644315594438
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = scriptNode(\"dataSplit\",\"./datasplit/dataSplit.py\",[\"--split_data\", in_data],\r\n",
        "                  [raw_data],[in_data],\"./preprocess_data\")\r\n",
        "                  \r\n",
        "step2 = scriptNode(\"preprocess_training_data\",\"train.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--output-data\", train],\r\n",
        "                    [in_data],[train],\"./preprocess_data\")\r\n",
        "\r\n",
        "step3 = scriptNode(\"preprocess_test_data\",\"test.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--output-data\", test],\r\n",
        "                    [in_data],[test],\"./preprocess_data\")\r\n",
        "\r\n",
        "step4 = scriptNode(\"preprocess_valid_data\",\"valid.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--output-data\", valid],\r\n",
        "                    [in_data],[valid],\"./preprocess_data\")\r\n",
        "                    \r\n",
        "step5 = scriptNode(\"Train GEFA\",\"train_test.py\",\r\n",
        "                    [\"--data-path\",davis1.as_mount(), \"--model_path\",model_output,\r\n",
        "                    \"--data_type\",0],\r\n",
        "                    None,[model_output],\"./GEFA\")\r\n",
        "\r\n",
        "step6 = scriptNode(\"Train GGNN\",\"main.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--train_dir\",train,\"--test_dir\",test,\r\n",
        "                     \"--valid_dir\",valid,\"--job-dir\",train_job_dir],\r\n",
        "                    [in_data,train,test,valid],[train_job_dir],\"./train_model\")\r\n",
        "\r\n",
        "step7 = scriptNode(\"R-Learn\",\"main.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--train_dir\",train,\"--test_dir\",test,\r\n",
        "                     \"--valid_dir\",valid,\"--job-dir\",finetune_job_dir,\"--data_path\",qsar.as_mount(),\r\n",
        "                     \"--trained\",train_job_dir],\r\n",
        "                    [in_data,train,test,valid,train_job_dir],[finetune_job_dir],\"./finetune_model\")\r\n",
        "\r\n",
        "\r\n",
        "step8 = scriptNode(\"Generate molecules\",\"main.py\",\r\n",
        "                    [\"--input-data\",in_data, \"--job-dir\",finetune_job_dir,\"--train_dir\",train,\r\n",
        "                    \"--generation\",generation,\"--trained\",train_job_dir,\"--data_path\",qsar.as_mount()],\r\n",
        "                    [in_data,finetune_job_dir,train,train_job_dir],[generation],\"./generate_molecule\")\r\n",
        "\r\n",
        "\r\n",
        "step9 = scriptNode(\"Prepare data for GEFA\",\"prepare_data.py\",\r\n",
        "                    [\"--data\",modified_csv, \"--pred\",pred1.as_mount(),\r\n",
        "                    \"--generation\",generation],\r\n",
        "                    [generation],[modified_csv],\"./generate_molecule\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "step10 = scriptNode(\"Predict Binding score\",\"predict.py\",\r\n",
        "                    [\"--data-path\",pred1.as_mount(), \"--model_path\",model_output,\r\n",
        "                    \"--data_type\",1,\"--out_path\",predict,\r\n",
        "                    \"--generated\",modified_csv],\r\n",
        "                    [model_output,modified_csv],[predict],\"./GEFA\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644315594647
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [step1,step2,step3,step4,step5,step6,step7,step8,step9,step10]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644315594824
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline_run1 = Experiment(ws, 'RL-Model').submit(pipeline, regenerate_outputs=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644315610032
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs = Environment.list(workspace=ws)\r\n",
        "\r\n",
        "for env in envs:\r\n",
        "    if env.startswith(\"AzureML\"):\r\n",
        "        print(\"Name\",env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644315610456
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}