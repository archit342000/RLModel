{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment, Datastore,Environment\n",
        "from azureml.widgets import RunDetails\n",
        " \n",
        "from azureml.core import Dataset\n",
        " \n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        " \n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        " \n",
        "from azureml.core.runconfig import RunConfiguration, DockerConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        " \n",
        "from azureml.core.model import Model\n",
        "\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        " \n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.37.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1643706269273
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1643706269672
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def_blob_store = ws.get_default_datastore()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1643706269897
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aml_compute_target = \"RLModel\"\n",
        "try:\n",
        "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "    print(\"found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    print(\"creating new compute target\")\n",
        "    \n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_ND6S\",\n",
        "                                                                min_nodes = 0, \n",
        "                                                                max_nodes = 3)    \n",
        "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=60)\n",
        "    \n",
        "print(\"Azure Machine Learning Compute attached\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "found existing compute target.\nAzure Machine Learning Compute attached\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1643706270874
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aml_run_config = RunConfiguration()\n",
        "\n",
        "#docker_config = DockerConfiguration(use_docker=True)\n",
        "#aml_run_config.docker = docker_config\n",
        "\n",
        "aml_run_config.target = aml_compute\n",
        "aml_run_config.environment.docker.enabled = True\n",
        "aml_run_config.environment.docker.base_image = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220113.v1\"\n",
        " \n",
        "aml_run_config.environment.python.user_managed_dependencies = False\n",
        " \n",
        "aml_run_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path='./environment.yml')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1643706271069
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scriptNode(name,script,arguments = None, inputs=None,outputs=None,source=None):   \n",
        "    nodestep = PythonScriptStep(name=name, script_name=script, arguments = arguments,\n",
        "                         inputs = inputs, outputs = outputs, compute_target=aml_compute,\n",
        "                         runconfig=aml_run_config, source_directory=source,\n",
        "                         allow_reuse=True)\n",
        "    return nodestep"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1643706271203
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#file_dataset = Dataset.File.upload_directory(src_dir=\"./pred_data\",target=def_blob_store)\n",
        "#file_dataset.register(workspace=ws,name='pred')\n",
        "\n",
        "#davis_data = Dataset.File.upload_directory(src_dir=\"./davis_data\",target=def_blob_store)\n",
        "#davis_data.register(workspace=ws,name='davis')\n",
        "\n",
        "#qsar_data = Dataset.File.upload_directory(src_dir=\"./qsar\",target=def_blob_store)\n",
        "#qsar_data.register(workspace=ws,name='qsar')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1643706271325
        },
        "scrolled": true,
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moses = Dataset.get_by_name(ws, name='moses')\n",
        "pred_dataset = Dataset.get_by_name(ws, name='pred')\n",
        "davis_dataset = Dataset.get_by_name(ws, name='davis')\n",
        "qsar_dataset = Dataset.get_by_name(ws,\"qsar\")"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1643706271539
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = moses.as_named_input('raw_data')\r\n",
        "in_data = PipelineData(\"Input_Data\", datastore=def_blob_store)\r\n",
        "\r\n",
        "train = PipelineData(\"training_processed\", datastore=def_blob_store)\r\n",
        "test = PipelineData(\"test_processed\", datastore=def_blob_store)\r\n",
        "valid = PipelineData(\"valid_processed\", datastore=def_blob_store)\r\n",
        "train_job_dir = PipelineData(\"train_job_dir\", datastore=def_blob_store)\r\n",
        "\r\n",
        "finetune_job_dir = PipelineData(\"finetune\", datastore=def_blob_store)\r\n",
        "\r\n",
        "qsar = qsar_dataset.as_named_input('qsar')\r\n",
        "\r\n",
        "generation = PipelineData(\"generation\", datastore=def_blob_store)\r\n",
        "\r\n",
        "modified_csv = PipelineData(\"csv\", datastore=def_blob_store)\r\n",
        "\r\n",
        "pred1 = pred_dataset.as_named_input('pred')\r\n",
        "\r\n",
        "davis1 = davis_dataset.as_named_input('davis')\r\n",
        "\r\n",
        "model_output = PipelineData(\"model\", datastore=def_blob_store)\r\n",
        "\r\n",
        "predict = PipelineData(\"predict\", datastore=def_blob_store)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1643706271628
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = scriptNode(\"dataSplit\",\"./datasplit/dataSplit.py\",[\"--split_data\", in_data],\r\n",
        "                  [raw_data],[in_data],\"./preprocess_data\")\r\n",
        "                  \r\n",
        "step2 = scriptNode(\"preprocess_training_data\",\"train.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--output-data\", train],\r\n",
        "                    [in_data],[train],\"./preprocess_data\")\r\n",
        "\r\n",
        "step3 = scriptNode(\"preprocess_test_data\",\"test.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--output-data\", test],\r\n",
        "                    [in_data],[test],\"./preprocess_data\")\r\n",
        "\r\n",
        "step4 = scriptNode(\"preprocess_valid_data\",\"valid.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--output-data\", valid],\r\n",
        "                    [in_data],[valid],\"./preprocess_data\")\r\n",
        "                    \r\n",
        "step5 = scriptNode(\"Train GEFA\",\"train_test.py\",\r\n",
        "                    [\"--data-path\",davis1.as_mount(), \"--model_path\",model_output,\r\n",
        "                    \"--data_type\",0],\r\n",
        "                    None,[model_output],\"./GEFA\")\r\n",
        "\r\n",
        "step6 = scriptNode(\"Train GGNN\",\"main.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--train_dir\",train,\"--test_dir\",test,\r\n",
        "                     \"--valid_dir\",valid,\"--job-dir\",train_job_dir],\r\n",
        "                    [in_data,train,test,valid],[train_job_dir],\"./train_model\")\r\n",
        "\r\n",
        "step7 = scriptNode(\"R-Learn\",\"main.py\",\r\n",
        "                    [\"--input-data\",in_data,\"--train_dir\",train,\"--test_dir\",test,\r\n",
        "                     \"--valid_dir\",valid,\"--job-dir\",finetune_job_dir,\"--data_path\",qsar.as_mount(),\r\n",
        "                     \"--trained\",train_job_dir],\r\n",
        "                    [in_data,train,test,valid,train_job_dir],[finetune_job_dir],\"./finetune_model\")\r\n",
        "\r\n",
        "\r\n",
        "step8 = scriptNode(\"Generate molecules\",\"main.py\",\r\n",
        "                    [\"--input-data\",in_data, \"--job-dir\",finetune_job_dir,\"--train_dir\",train,\r\n",
        "                    \"--generation\",generation,\"--trained\",train_job_dir,\"--data_path\",qsar.as_mount()],\r\n",
        "                    [in_data,finetune_job_dir,train,train_job_dir],[generation],\"./generate_molecule\")\r\n",
        "\r\n",
        "\r\n",
        "step9 = scriptNode(\"Prepare data for GEFA\",\"prepare_data.py\",\r\n",
        "                    [\"--data\",modified_csv, \"--pred\",pred1.as_mount(),\r\n",
        "                    \"--generation\",generation],\r\n",
        "                    [generation],[modified_csv],\"./generate_molecule\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "step10 = scriptNode(\"Predict Binding score\",\"predict.py\",\r\n",
        "                    [\"--data-path\",pred1.as_mount(), \"--model_path\",model_output,\r\n",
        "                    \"--data_type\",1,\"--out_path\",predict,\r\n",
        "                    \"--generated\",modified_csv],\r\n",
        "                    [model_output,modified_csv],[predict],\"./GEFA\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1643706271807
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [step1,step2,step3,step4,step5,step6,step7,step8,step9,step10]"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1643706271987
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline_run1 = Experiment(ws, 'RL-Model').submit(pipeline, regenerate_outputs=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step dataSplit [a0026dfd][c537d811-8b66-4f15-98b8-4a1fa770332c], (This step is eligible to reuse a previous run's output)\nCreated step preprocess_training_data [417b2121][d2fe58b6-505b-4f91-a621-38090c074017], (This step is eligible to reuse a previous run's output)\nCreated step preprocess_test_data [839b09e1][31df9851-c145-44dd-9b8b-69101c2f1133], (This step is eligible to reuse a previous run's output)\nCreated step preprocess_valid_data [66f6d3c2][b6a8820e-6001-4cf2-b86c-0b3aa59da4ac], (This step is eligible to reuse a previous run's output)\nCreated step Train GEFA [89e87144][9078b131-f3a5-4db5-8456-676decc298b0], (This step will run and generate new outputs)\nCreated step Train GGNN [29107c6c][6f77156e-28fe-48da-a6d9-1db324995768], (This step is eligible to reuse a previous run's output)\nCreated step R-Learn [76944a42][aee573e7-41bf-436c-998c-5e7b799ee6e6], (This step is eligible to reuse a previous run's output)\nCreated step Generate molecules [71d333ae][ba92fb94-6488-4c56-8d6e-40f306dfef81], (This step is eligible to reuse a previous run's output)\nCreated step Prepare data for GEFA [6a6a7819][6957e4c5-a3f0-44f0-b3cb-be56a1f6d8a4], (This step is eligible to reuse a previous run's output)\nCreated step Predict Binding score [91b05fff][5b4c85a4-eba4-45fa-9c06-fb82121d47de], (This step will run and generate new outputs)\nSubmitted PipelineRun 19d1a26d-f5ba-46d4-bc02-91a1063f67b7\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/19d1a26d-f5ba-46d4-bc02-91a1063f67b7?wsid=/subscriptions/a7dcae9c-3a2e-459d-bc9c-55d04b3aa0b9/resourcegroups/rl_model/workspaces/rl_model&tid=85955b62-b456-42f2-b402-4d658744d938\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1643706286559
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs = Environment.list(workspace=ws)\r\n",
        "\r\n",
        "for env in envs:\r\n",
        "    if env.startswith(\"AzureML\"):\r\n",
        "        print(\"Name\",env)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name AzureML-PyTorch-1.3-CPU\nName AzureML-Dask-CPU\nName AzureML-Dask-GPU\nName AzureML-VowpalWabbit-8.8.0\nName AzureML-Triton\nName AzureML-mlflow-ubuntu18.04-py37-cpu-inference\nName AzureML-minimal-ubuntu18.04-py37-cpu-inference\nName AzureML-pytorch-1.6-ubuntu18.04-py37-cpu-inference\nName AzureML-onnxruntime-1.6-ubuntu18.04-py37-cpu-inference\nName AzureML-xgboost-0.9-ubuntu18.04-py37-cpu-inference\nName AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference\nName AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\nName AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu\nName AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11.0.3-gpu-inference\nName AzureML-tensorflow-1.15-ubuntu18.04-py37-cpu-inference\nName AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\nName AzureML-pytorch-1.7-ubuntu18.04-py37-cpu-inference\nName AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference\nName AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu\nName AzureML-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu\nName AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu\nName AzureML-pytorch-1.8-ubuntu18.04-py37-cuda11-gpu\nName AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1643706286873
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}